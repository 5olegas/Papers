Chapter 8 - Optimization for Training Deep Models
- Might be using indirect loss function as compared to pure optimization problem;
- Minibatch is selected randomly! In multiple epoch training, after the first pass, estimates become biased; However, in practice, the additional epoches usually provide enough benefit as compared to the harm it brings;
- Chanllenges in NN optimization: ill-conditioning; multiple local minima corresponding to modl identifiability; plateaus, saddle points and flat regions-saddle points are more often using the n-coin toss case; Second-oder methods not so popular may due to this saddle point problem like in Newton's methods; Cliffs or very large gradients; long term dependency like in RNN which may cause vanishing or exploding gradient problem; Inexact gradients; Local structure may not indicate the global structure as shown in figure 8.4; 
- SGD: pay attention to learning rate episolon_k; computation per update is constant; rate of convergence is O(1/sqrt(k)) for SGD but O(1/k) for BGD;
- Momemtum: accumulates an decaying moving average of previous gradients, offset the oscillation not pointing to the minimum; 1/(1-alpha) can treated as the step size when convergence reached; we may think of it as a particle subject to continuous-time Newtonian dynamics with gradient being the velosity;
- Nesterov Momentum: using the gradient with "correctly updated" parameter value;
- Parameter initialization: Breaking symmetry between hidden units; weights usually sampled from Gaussian or Uniform; Larger initial weights may yield stronger symmetry-breaking effect; In terms of regularization, initilization may treated differently, they encurage smaller ones as final result stay in regions near the initial parameters; 
- However, optimal criteria for initial weights often do not lead to optimal performance: wrong criteria; properties not persist; just improving the speed;
- If computation power allowed, initialization can be treated as hyperparameter searching with one minibatch;
- Mostly, bias is set to zero except: a bias is an output unit which can be set to its marginal statistics; to avoid too much saturation; functioned as a gate;
- Adaptive Learning Rates
  - AdaGrad: larger partial derivative corresponds to rapid decrease of learning rate;
  - RMSProp: exponentially decaying average to discard history from extreme past;
  - Adam: adaptive momemtum combining both momentum and RMSProp;
- Second-order Methods
  - Newton's Method: adding alpha along diagnal of Hessian to ensure Positive Definite;
  - Conjugate Gradients: d_t^T H d_{t-1} = 0 to solve the best beta_t which satisfiest d_t = g + beta_t d_{t-1}
  - BFGS: approximate H^{-1} with a matrix M_t - theta_{t+1} = theta_t + epsilon * Mt g_t;
- Batch Normalization: SGD is under the assumption that other parameters are constant while updating W_i. However in deep models this hardly holds. apply affine transformation on top of BN to keep the learning power; Apply BN on WX as X is less ammenable and more non-Gaussian;
- Coordinate Descent; Polyak Averaging - Theta_t = alpha Theta_{t-1} + (1-alpha)Theta_t;
- Supervised pretraing: finetuning; transfer learning; FitNets using pretraining to guide the target network's hidden layer;
- Continuation Methods and Curriculum Learning;
