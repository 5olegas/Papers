# Generative Adversarial Networks

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, NIPS, 2014

## Summary
Quite new idea of training a generative model to fit the data generation distribution. A discriminator network is configured to distinguish between real training examples and ones counterfeited by the generator network. Its goal is to detect the fake ones generated by the generator. The generative network is trained to fool the discriminator. So at final equilibrium, discriminator will output 1/2 for all input examples including both training data and fake ones coined from the generator.

- Model
  - min_G max_D V(G, D) = E_{x in p_data}[log D(x)] + E_{z in p_z}[log(1-D(G(z)))];
  - k steps of optimizing D and one step of optimizing G, analogous to SML/PCD;
  - Early in learning, log( 1-D(G(z)) ) is saturate. So it might be better to train with log(D(G(z))), which provides much stronger gradients;
  - Given some G, we can reach global maximum when D(x) = p_data / (p_data + p_g);
  - Given D^*(x), the global minimum can be approached if p_g = p_data, which results in the minima of -log4;
  - In Algorithm 1 of the paper, if k steps of updating D allows it to reach its optimum, then after updating G, p_g will converge to p_data; (a little bit abstract to comprehend)

- Dataset
  - MNIST
  - TFD, the Toronto Face Database
  - CIFAR-10
  
- Experiments
  - Generator, mixture of ReLu and Sigmoid, noise at bottommost layer;
  - Discriminator, maxout, dropout applied;
  - Gaussian Parzen Window for test set data log-likelihood reporting;
  - Generating samples for the above datasets, nearest neighbor comparison;

## Strengths
  - Great original idea of training generative models, which can make use of backpropagation based training as discriminative network is incorporated to provide the supervision;
  - Statistical advantages might also be learnt as no exact training examples are updated during the generative network, which can allow very sharp or degenerate distributions;
  - Features learned from generative or discriminator nets might be transferred for other task's performance improvement;

## Weaknesses/Notes
  - Interpretation is lacking for generative networks. As mentioned in conclusion and future work, can we use p(x|c), i.e. c, to guide the learning. Or use learned approximate inference to explain the hidden units;
  
## Notes from NIPS 2016 GAN tutorial [[TechReport](https://arxiv.org/abs/1701.00160)]
  - Taxonomy of deep generative models like explicity and implicit density models
  - As of late 2016, simultaneous gradient descent with one step for each player
  - MLE game interpretation of GAN
  - JSD cost function IS NOT the distinguishable features for GAN, which choose to generate a small number of modes
  - Non-saturating heuristics gives low variance of cost function with high gradients being from fake samples
  - Training with labels can produce samples of higher quality; One-sided lable smoothing for extreme probabilities
  - Reference batch normalization and virtual batch normalization for mitigating correlation problems between samples
  - Balance between G and D: encourage D to be optimized first as theoretical proof for convergence
  - Mode collapse originates from the gap between min max and max min
  - Minibatch features approach to deal with mode collapse problem, but results in counting, perspective and global structure problems
  - unrolled GAN: backpropagation through the maximization operation, k steps of learning in D(x), backpropagate through all k steps of learning when updating G(z)
  - It's hard to evaluate generative models
  - For discrete outputs: training samples of continuous values that can be decoded to discrete ones, like word embeddings;
  - Semi-supervised learning: feature matching GANs, turning a classification of n classes into n+1 classes
  - Application of latent code z for producing the images, but hard to infer P(z|x) as it's implicit density; we may need another network to approximate it; InfoGAN trained the code z to be more useful by high mutual information between z and x
