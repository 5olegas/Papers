# Generative Adversarial Networks

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, NIPS, 2014

## Summary
Quite new idea of training a generative model to fit the data generation distribution. A discriminator network is configured to distinguish between real training examples and ones counterfeited by the generator network. Its goal is to detect the fake ones generated by the generator. The generative network is trained to fool the discriminator. So at final equilibrium, discriminator will output 1/2 for all input examples including both training data and fake ones coined from the generator.

- Model
  - min_G max_D V(G, D) = E_{x in p_data}[log D(x)] + E_{z in p_z}[log(1-D(G(z)))];
  - k steps of optimizing D and one step of optimizing G, analogous to SML/PCD;
  - Early in learning, log( 1-D(G(z)) ) is saturate. So it might be better to train with log(D(G(z))), which provides much stronger gradients;
  - Given some G, we can reach global maximum when D(x) = p_data / (p_data + p_g);
  - Given D^*(x), the global minimum can be approached if p_g = p_data, which results in the minima of -log4;
  - In Algorithm 1 of the paper, if k steps of updating D allows it to reach its optimum, then after updating G, p_g will converge to p_data; (a little bit abstract to comprehend)

- Dataset
  - MNIST
  - TFD, the Toronto Face Database
  - CIFAR-10
  
- Experiments
  - Generator, mixture of ReLu and Sigmoid, noise at bottommost layer;
  - Discriminator, maxout, dropout applied;
  - Gaussian Parzen Window for test set data log-likelihood reporting;
  - Generating samples for the above datasets, nearest neighbor comparison;

## Strengths
  - Great original idea of training generative models, which can make use of backpropagation based training as discriminative network is incorporated to provide the supervision;
  - Statistical advantages might also be learnt as no exact training examples are updated during the generative network, which can allow very sharp or degenerate distributions;
  - Features learned from generative or discriminator nets might be transferred for other task's performance improvement;

## Weaknesses/Notes
  - Interpretation is lacking for generative networks. As mentioned in conclusion and future work, can we use p(x|c), i.e. c, to guide the learning. Or use learned approximate inference to explain the hidden units;
  - 
