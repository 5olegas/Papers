Chapter 20 - Deep Generative Models
- Boltzmann Machines: originally for binary vectors; energy based model p(x) = exp(-E(x))/Z, E(x) = -x^T U x - b^T x; Divide x into visible and hidden variables v and h, E(v,h) = -v^T R v - v^T W h - h^T S h - b^v - c^T h; learning rule is local just as axons and dendrites firing locally;
- Restricted Boltzmann Machines: undirected connection with one hidden and one visible layer, E(v,h) = -b^T v - c^T h - v^T W h; p(h|v) and p(v|h) can be expressed explicitly; derivation of updating (see Hinton's practical guide([Practical Guide to RBM](http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)) and my own [[pic](https://github.com/yufengm/Papers/edit/master/reviews/rbm_update_derivation.jpg)]); block Gibbs sampling combined with CD or SML;
- Deep Belief Networks: fell out of favor and are rarely used today; every unit in each layer is connected to every unit in each neighboring layer; top two layers are undirected while the rest is directed with arrows coming from top to bottom pointing toward data layer; energy function see 20.17-19; training begins with the bottom RBM to maximize E_v log p(v), next we train with the successive RBM to maximize p^1(h^1), ...; Generative fine-tuning with wake-sleep algorithm; Initialize MLP with the weights trained from DBM for classification task - example of discriminative fine-tuning;
- Deep Boltzmann Machine: entirely undirected model; within each layer, each variable is mutually conditionally independent; joint probability of visible and latent variables is defined in 20.24; still bipartite structure;
  - Properties: lack of intralayer interactions - fixed-point equation;
  - DBM Mean Field Inference: minimize KL(Q||P); in binary hidden units version: get 20.33~34 for iterative fixed-point equation updates;
  - DBM Parameter Learning: see details in [[pic](https://github.com/yufengm/Papers/edit/master/reviews/dbm_training_update_derivation.jpg)]
  - Layer-wise pretraining: random initialization usually results in failure; each layer is trained in isolation as a RBM, after all is trained, DBM is jointly trained with all layers via PCD; not quite understand the two copy explanation in P662;
  - Jointly Training DBM: centered deep BM, replace x with x - \mu, in which x \approx \mu with initialization, Hessian matrix is better conditioned; Multi-prediction DBM, mean field equations as a family of RNN solving inference problem with predic missing values in data, allowing back-propagation directly in DBM as compared to approximation in SML, which can result in more accurate classifier on its own; relationship with dropout, in MP-DBM, unobserved unit in visible variables is treated as latent ones;
- Boltzmann Machines for Real-Valued Data: common methods-treat real value in interval [0,1] as expectation of binary values
  - Gaussian-Bernoulli RBMs: binary hidden and real-value visible units; visible's mean is a function of hidden units; Equa. 20.38; Derivation of p(h_i=1|v) similar as in 20.7; Energy function as Equation. 20.42;
  - Handling Conditional Covariance
    - Mean and Covariance RBM, mean units and covariance units, combination of two energy functions, difficult to train with CD or PCD as inverse structure of covariance matrix, Hamiltonian Monte Carlo methods; 
    - mPoT: Gaussian-Bernoulli energy function + conditionally independent Gamma distributions as like 20.48, training with Hamiltonian Monte Carlo;
    - ssRBMs: binary spike units h and real-valued slab units s; some settings may cause a covariance matrix that is not positive definite; allows for higher-order interactions;
- Convolutional Boltzmann Machines: probabilistic max pooling, the detector units at most one may be active at a time with only n+1 states; hard to deal with different spatial sizes of pooling;
- BM for Structured or Sequential Outputs: structured outputs and sequence modeling; PGM of character's movement; RNN-RBM for song composition, in which RNN emits parameters for each time step;
- Other Boltzmann Machines: extended with different training criteria besides log p(v); higher order BM;
- BP through Random Operations: introduce source of randomness on random variables like treating y from p(y|w) as y=f(z;w) where z is the source of randomness, which is called reparameterization trick, z is typically sampled from simple distribution like uniform or unit Gaussian; application like DAE or NN with dropout, in which stochastic noise is taken as an input;
  - BP through Discrete Stochastic Operations: with discrete y, transformation f might be a step function, whose derivative is either undefined or zero, which is not useful for updating the model parameters; Core idea: expectation of J is often a smooth function, even though intractable, we can still approximate it with a Monte Carlo average; See Equation. 20.62 for derivation, which is unbiased but with high variance; Variance Reduction: offset baseline to reduce variance, b is usually obtained from adding extra output in NN, which is trained with MSE w.r.t. the two terms in 20.68;
