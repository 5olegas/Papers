Chapter 20 - Deep Generative Models
- Boltzmann Machines: originally for binary vectors; energy based model p(x) = exp(-E(x))/Z, E(x) = -x^T U x - b^T x; Divide x into visible and hidden variables v and h, E(v,h) = -v^T R v - v^T W h - h^T S h - b^v - c^T h; learning rule is local just as axons and dendrites firing locally;
- Restricted Boltzmann Machines: undirected connection with one hidden and one visible layer, E(v,h) = -b^T v - c^T h - v^T W h; p(h|v) and p(v|h) can be expressed explicitly; derivation of updating (see Hinton's practical guide([Practical Guide to RBM](http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)) and my own [[pic](https://github.com/yufengm/Papers/edit/master/reviews/rbm_update_derivation.jpg)]); block Gibbs sampling combined with CD or SML;
- Deep Belief Networks: fell out of favor and are rarely used today; every unit in each layer is connected to every unit in each neighboring layer; top two layers are undirected while the rest is directed with arrows coming from top to bottom pointing toward data layer; energy function see 20.17-19; training begins with the bottom RBM to maximize E_v log p(v), next we train with the successive RBM to maximize p^1(h^1), ...; Generative fine-tuning with wake-sleep algorithm; Initialize MLP with the weights trained from DBM for classification task - example of discriminative fine-tuning;
- Deep Boltzmann Machine: entirely undirected model; within each layer, each variable is mutually conditionally independent; joint probability of visible and latent variables is defined in 20.24; still bipartite structure;
  - Properties: lack of intralayer interactions - fixed-point equation;
  - DBM Mean Field Inference: minimize KL(Q||P); in binary hidden units version: get 20.33~34 for iterative fixed-point equation updates;
  - DBM Parameter Learning: see details in [[pic](https://github.com/yufengm/Papers/edit/master/reviews/dbm_training_update_derivation.jpg)]
  - Layer-wise pretraining: random initialization usually results in failure; each layer is trained in isolation as a RBM, after all is trained, DBM is jointly trained with all layers via PCD; not quite understand the two copy explanation in P662;
  - Jointly Training DBM: centered deep BM, replace x with x - \mu, in which x \approx \mu with initialization, Hessian matrix is better conditioned; Multi-prediction DBM, mean field equations as a family of RNN solving inference problem with predic missing values in data, allowing back-propagation directly in DBM as compared to approximation in SML, which can result in more accurate classifier on its own; relationship with dropout, in MP-DBM, unobserved unit in visible variables is treated as latent ones;
- Boltzmann Machines for Real-Valued Data: common methods-treat real value in interval [0,1] as expectation of binary values
  - Gaussian-Bernoulli RBMs: binary hidden and real-value visible units; visible's mean is a function of hidden units; Equa. 20.38; Derivation of p(h_i=1|v) similar as in 20.7; Energy function as Equation. 20.42;
  - Handling Conditional Covariance
    - Mean and Covariance RBM, mean units and covariance units, combination of two energy functions, difficult to train with CD or PCD as inverse structure of covariance matrix, Hamiltonian Monte Carlo methods; 
    - mPoT: Gaussian-Bernoulli energy function + conditionally independent Gamma distributions as like 20.48, training with Hamiltonian Monte Carlo;
    - ssRBMs: binary spike units h and real-valued slab units s; some settings may cause a covariance matrix that is not positive definite; allows for higher-order interactions;
- Convolutional Boltzmann Machines: probabilistic max pooling, the detector units at most one may be active at a time with only n+1 states; hard to deal with different spatial sizes of pooling;
- BM for Structured or Sequential Outputs: structured outputs and sequence modeling; PGM of character's movement; RNN-RBM for song composition, in which RNN emits parameters for each time step;
- Other Boltzmann Machines: extended with different training criteria besides log p(v); higher order BM;
- BP through Random Operations: introduce source of randomness on random variables like treating y from p(y|w) as y=f(z;w) where z is the source of randomness, which is called reparameterization trick, z is typically sampled from simple distribution like uniform or unit Gaussian; application like DAE or NN with dropout, in which stochastic noise is taken as an input;
  - BP through Discrete Stochastic Operations: with discrete y, transformation f might be a step function, whose derivative is either undefined or zero, which is not useful for updating the model parameters; Core idea: expectation of J is often a smooth function, even though intractable, we can still approximate it with a Monte Carlo average; See Equation. 20.62 for derivation, which is unbiased but with high variance; Variance Reduction: offset baseline to reduce variance, b is usually obtained from adding extra output in NN, which is trained with MSE w.r.t. the two terms in 20.68;
- Directed Generative Nets: DBN, partially directed; Sparse Coding, shallow directed generative models.
  - SBN: binary vector, each state is influenced by all ancestor states; generating sample is efficient while inference is intractable even with mean field as taking expectation of cliques needs to include all layers; Learned inference to perform inference as like week-sleep;
  - Differentiable Generator Networks: identify the source distribution and apply some nonlinear transformation, build connections between these two distributions via Equation. 20.73; Use g to define a conditional distribution over sample variable x, then take expectation to get the marginal distribution; Specify the correspondence between z and x as like in supervised learning with x and y, like the rendering example in P686;
  - VAE: train a parametric encoder to produce parameters of q, if z is continuous, we can use back-propogation to learn the parameters, which makes learning as just MLE w.r.t. parameters; Samples from VAE might be blurry; VAE fail to use all set of latent variables of z; DRAW model, squentially draw pixel values via RNN; Importance-weighted Autoencoder; Manifold learning indication;
  - GAN: discriminator network aims at distinguish between real and fake, while generator network tries to fool the discriminator; Minmax problem, with 1/2 for d(x) while at convergence; Reformulate objective function as MLE when d(x) is optimal; g aims to increase log-probability of d(x) making mistakes, which is the best-performing formulation of GAN; DCGAN; LAPGAN; GAN can fit probability distributions that assign zero probability to training points; Dropout is important in discriminator nets;
  - Generative Moment Matching nets: moment matching, in which moment is defined as expectation of different powers of a random varible; Maximum Mean Discrepancy, which measures the error in the first moments in infinite-dimensional space; Doesnot allow per example training;
  - Convolutional Generative Networks: un-pooling operation by Dosovitskiy et al;
  - Auto-Regressive Networks: no latent variables, decompose joint probability using chain rules; been called as FVBNs;
  - Linear Auto-Regressive Networks: P(x_i | x_1, ..., x_{i-1}) is parametrized as a linear model; Generalization of linear classification methods to generative modeling;
  - Neural Auto-Regressive Networks: using neural network to approximate P(x_i|x_1,...,x_{i-1}); hidden features can be reused for predicting following visible variables; NN predicts parameters of the conditional distribution of x_i;
  - NADE: parameter sharing scheme as in Figure 20.10; Pretty much like the mean field inference in RBM, while differentiates in hidden-to-output parametrization; Real-valued NADE, with softmax unit emitting mixture weights and variances being parametrized; Ensemble of arbitary order models, which generalizes better;
- Drawing Samples from Autoencoders: autoencoders can learn the data distribution in some way; VAE vis ancestral sampling; CAE induce a random walk along the surface of some manifold;
  - Markov Chain with DAE: start with x, noise injection and sample x' from C(x'|x); encode h = f(x'); decode h to produce parameters of p(x|w=g(h)); sample next state x;
  - Clamping and Conditional Sampling: conditional sample x_f given observed x_o and other latent variables;
  - Walk-Back Training Procedure: multiple stochastic encode-decode steps initialized at a training example, with penality placed on the last probablistic reconstructions;
- GSN: generalization of denoising autoencoders, which includes hidden variables h during Markov chain; parametrized by two conditional probability distributions, each specify one step of Markov chain; Difference between GSN and classical PGMs, parametrize the generative process while in traditional PGMs it was specified by joint distribution of both visible and hidden variables; Trained with reconstruction log-probability on visible units;
  - Discriminant GSNs: modify the framework to optimize p(y|x);
- Other Generation Schemes: diffusion inversion fromnonequilibrium thermodynamics, gradually restore the probability distribution as it approaches the target one; approximate Bayesian computation, samples are rejected or modified to make the moments of selected functions of the samples match desired distribution;
- Evaluation of Generative Models: practical criterion specific to task of interest; hard to build criterions that models are compared fairly like the AIS method for estimation of log Z; Preprocessing step like the binarization; Visually inspecting generated samples; Evaluate log-likelihood when it is computationally feasible; The choice of metrics must match the intended use of the model;
- Conclusion: generative models help make us understand the representation of data better; can offer many different ways of representing x by taking expectations of h at different layers of the hierarchy;
